{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. A data wrangler is a person who performs these transformation operations.\n",
    "\n",
    "Some operations can be:\n",
    "\n",
    "1. Pre-processing\n",
    "2. Dealing missing data\n",
    "3. Data formatting\n",
    "4. Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "is an approach to analyze data in order to summarize the their main characteristics, often with visual methods. In particular: to gain better understanding of the dataset, to encover relationships between variables and to extract important information from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a statistical metrics for measuring the relationship between two different variables/features. Correlation doesn't imply causation.\n",
    "\n",
    "There are 2 parameters to measure the strength of the correlation:\n",
    "\n",
    "1. Correlation Coefficient:\n",
    "\n",
    "> measures the linear dependence between 2 variables.\n",
    ">\n",
    ">  3 results:\n",
    ">\n",
    "> - Strong Positive relationship (result is **close to +1**)\n",
    ">\n",
    "> - Strong Negative relationship (result is **close to -1**)\n",
    ">\n",
    "> - No relationship (result is **close to 0**)\n",
    "\n",
    "2. P-value:\n",
    "\n",
    "> shows whether the obtained result (correlation coefficient) is statistically significant.\n",
    ">\n",
    "> 4 results:\n",
    ">\n",
    "> - Strong certainty about the correlation coefficient (result is **<0.001**)\n",
    ">\n",
    "> - Moderately certainty about the correlation coefficient (result is **between 0.001 and 0.05**)\n",
    ">\n",
    "> - Weak certainty about the correlation coefficient (result is **between 0.05 and 0.1**)\n",
    ">\n",
    "> - No certainty about the correlation coefficient (result is **>0.1**)\n",
    "\n",
    "For strong correlation:\n",
    "\n",
    "- correlation coefficient should be close to +1 or close to -1\n",
    "\n",
    "- P-value should be <0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Python:\n",
    "\n",
    "By default, the function `corr( )` calculate the Pearson Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but, to know the significant of the correlation, we can use the function `pearsonr` in the module `stats` to obtain Pearson Coefficient and P-value:\n",
    "\n",
    "`pearson_coef, p_value = stats.pearsonr('feature 1','feature2')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g:\n",
    "from scipy import stats\n",
    "pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of variance (ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a statistical test and it can be used for finding correlation between different group of categorical variable.\n",
    "\n",
    "2 Parameters:\n",
    "\n",
    "1. F-test score:\n",
    "\n",
    ">\n",
    "> calculates the ratio of variation between groups mean, over the variation within each of the sample groups.\n",
    ">\n",
    "\n",
    "2. P-value: \n",
    "\n",
    ">\n",
    "> shows whether the obtained result (F-test score) is statistically significant.\n",
    ">\n",
    "\n",
    "For strong correlation:\n",
    "\n",
    "- large value of F-test score\n",
    "\n",
    "- small value of P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `f_oneway` in the module `stats`  to obtain the F-test score and P-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g\n",
    "\n",
    "# f_val, p_val = stats.f_oneway(<group of category1>, <group of category2>,...)\n",
    "\n",
    "from scipy import stats\n",
    "f_val, p_val = stats.f_oneway(df.get_group('fwd')['price'], df.get_group('rwd')['price'], df.get_group('4wd')['price'])  \n",
    "print( \"ANOVA results: F=\", f_val, \", P =\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model can be thought as a mathematical equation used to predict a velue given one or more other features.\n",
    "\n",
    "2 different ways to work with the model:\n",
    "\n",
    "- Supervise: to observe and direct the execution of the task, project or activity. \n",
    "\n",
    "\n",
    ">\n",
    "> we \"teaching\" the model, then with that knowledge, it can predict unknown or future instance.\n",
    ">\n",
    ">\n",
    "> Supervised model:\n",
    ">\n",
    "> 1. Regression (process of predicting continuous values)\n",
    "> 2. Classification (process of predicting discrete class labels or categories)\n",
    "\n",
    "- Unsupervise: the model works on its own to discover information (most difficult algorithms).\n",
    "\n",
    "\n",
    ">\n",
    "> Unsupervised model:\n",
    ">\n",
    "> 1. Dimension reduction\n",
    "> 2. Density Reduction\n",
    "> 3. Market basket Analysis\n",
    "> 4. Clustering (most popular)\n",
    "\n",
    "\n",
    "\n",
    "## Supervised Model\n",
    "\n",
    "## Regression\n",
    "\n",
    "### 1. Single Linear Regression (SLR): \n",
    "\n",
    "is a method to help us understand the relationship between two variables (predictor = indipendent variable x, target = dependent variable y). \n",
    "\n",
    "$$\n",
    "Yhat = b_0 + b_1 X_1 \n",
    "$$\n",
    "\n",
    "In Python:\n",
    "\n",
    "1. Step: train and test (an important step: split your data into training and testing data)\n",
    "\n",
    "2. Step: create a linear regression object\n",
    "\n",
    "3. Step: fit the model with training data\n",
    "\n",
    "4. Step: predict with traing data\n",
    "\n",
    "5. Step: calculate two important parameters to determine how accurate the model is:\n",
    "        - MSE (Mean Squared Error)\n",
    "        - R-squares (% of the variation of the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Step: train and test (an important step: split your data into training and testing data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=0)\n",
    "\n",
    "#2. Step: create a linear regression object\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "\n",
    "#3. Step: fit the model with training data\n",
    "lm.fit(x_train[['...']], y_train)   #e.g. lr.fit(x_train[['horsepower']], y_train)\n",
    "\n",
    "#4. Step: predict with traing data\n",
    "yhat_train = lm.predict(x_train[['...']])  #e.g. yhat_train = lr.predict(x_train[['horsepower']])\n",
    "yhat_train[0:5]` #show the result\n",
    "\n",
    "#Optional: calculate yhat_test with testing data and compare the values\n",
    "\n",
    "#5. Step: calculate two important parameters to determine how accurate the model is:\n",
    " \n",
    "#5.1 MSE:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df[['...']],y_hat) # e.g. mean_squared_error(df[['price']],y_hat)`\n",
    "\n",
    "#5.2 R-square:\n",
    "lm.score(x,y) #e.g. lm.score(x_train,y_train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiple Linear Regression (MLR):\n",
    "\n",
    "is a method used to explain the relationship between one continuous target y and 2 or more predictor X variables.\n",
    "\n",
    "\n",
    "$$\n",
    "Yhat = b_0 + b_1 X_1 +b_2 X_2 + b_3 X_3 + ... + b_n X_n\n",
    "$$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Step: train and test (an important step: split your data into training and testing data)\n",
    "\n",
    "2. Step: create a linear regression object\n",
    "\n",
    "3. Step: fit the model with training data\n",
    "\n",
    "4. Step: predict with traing data\n",
    "\n",
    "5. Step: calculate two important parameters to determine how accurate the model is:\n",
    "        - MSE (Mean Squared Error)\n",
    "        - R-squares (% of the variation of the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Step: train and test (an important step: split your data into training and testing data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=0)\n",
    "\n",
    "#2. Step: create a linear regression object\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "\n",
    "#3. Step: fit the model with training data\n",
    "lm.fit(x_train[['...','...','...']], y_train) \n",
    "#e.g. lm.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)\n",
    "\n",
    "#4. Step: predict with traing data\n",
    "yhat_train = lm.predict(x_train[['...','...','...']]) \n",
    "#e.g. yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "yhat_train[0:5]` #show the result\n",
    "#Optional: calculate yhat_test with testing data and compare the values\n",
    "\n",
    "#5. Step: calculate two important parameters to determine how accurate the model is:\n",
    "\n",
    "#MSE:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df[['...']],y_hat) # e.g. mean_squared_error(df[['price']],y_hat)\n",
    "#(where df[['...']] = actual value, y_hat = predicted value)\n",
    " \n",
    "#R_squared:\n",
    "lm.score(x,y) #e.g. lm.score(x_train,y_train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Polynomial Regression:\n",
    "\n",
    "is a particular case of the general linear regression model or multiple linear regression models.\n",
    "\n",
    "$$\n",
    "Yhat = b_0 + b_1 X^2 +b_2 X^2 + b_3 X^3 + ... + b_n X^n\n",
    "$$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Step: define x and y\n",
    "\n",
    "2. Step: fit the polynomial using the function `polyfit`, and use the function `poly1d` to display the polynomial function\n",
    "\n",
    "3. Step: plot the function\n",
    "\n",
    "If polynomial function gets complicated. E.g. degree=2:\n",
    "\n",
    "$$\n",
    "Yhat = a + b_1 X_1 +b_2 X_2 +b_3 X_1 X_2+b_4 X_1^2+b_5 X_2^2\n",
    "$$\n",
    "\n",
    "1. Step: create a PolynomialFeatures object\n",
    "\n",
    "2. Step: define parameter Z:\n",
    "\n",
    "3. Step: transform polynomial function in multiple features.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Step: define x and y\n",
    "\n",
    "x = df['...'] # e.g. x = df['highway-mpg']\n",
    "\n",
    "y = df['...'] # e.g. y = df['price']\n",
    "\n",
    "#2. Step: fit the polynomial using the function `polyfit`, and use the function `poly1d` to display the polynomial function\n",
    "f = np.polyfit(x, y, n) # e.g. f = np.polyfit(x, y, 3) where 3 is grade of polynomial function\n",
    "p = np.poly1d(f) # display polynomial function\n",
    "\n",
    "#3. Step: plot the function\n",
    "PlotPolly(p, x, y, '...') # e.g. PlotPolly(p, x, y, 'highway-mpg')   \n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#If polynomial function gets complicated. E.g. degree=2:\n",
    "\n",
    "\n",
    "#1. Step: create a PolynomialFeatures object\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pr = PolynomialFeatures(degree=2)\n",
    "\n",
    "#2. Step: define parameter Z:\n",
    "Z = df[['...', '...', '...', '...']] #e.g. Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n",
    "\n",
    "#3. Step: transform polynomial function in multiple features.  \n",
    "Z_pr=pr.fit_transform(Z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipelines\n",
    "\n",
    "is used for simplify the steps of processing the data.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Step: create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.\n",
    "\n",
    "2. Step: input the list as an argument to the pipeline constructor \n",
    "\n",
    "3. normalize the data,  perform a transform and fit the model simultaneously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Step: create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), \n",
    "       ('model',LinearRegression())]\n",
    "\n",
    "#2. Step: input the list as an argument to the pipeline constructor \n",
    "pipe = Pipeline(Input)\n",
    "\n",
    "#3. normalize the data,  perform a transform and fit the model simultaneously\n",
    "\n",
    "pipe.fit(... ,... ) #e.g. pipe.fit(Z,df[\"price\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Refinement\n",
    "\n",
    "find the best values of R_squared and MSE.\n",
    "\n",
    "\n",
    "### Better fit of the model to the data : \n",
    "\n",
    "- large R_squared\n",
    "- low MSE (not always mean that the model is fit!)\n",
    "\n",
    "### Underfitting: \n",
    "\n",
    "model is too simple to fit the data\n",
    "\n",
    "### Overfitting: \n",
    "\n",
    "model is too flexible ans fits the noise rather than the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ridge Regression (refinement)\n",
    "\n",
    "is a method that introduce the parameter alpha to prevents overfitting. \n",
    "\n",
    "In Python:\n",
    "\n",
    "1. Step: select the value of alpha that maximize the R-squared\n",
    "\n",
    "2. Step: fit the model\n",
    "\n",
    "3. Step: predict \n",
    "\n",
    "4. Step: calculate R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Step: select the value of alpha that maximize the R-squared\n",
    "from sklearn.linear_model import Ridge\n",
    "RidgeModel = Ridge(alpha = 0.1)\n",
    "\n",
    "#2. Step: fit the model\n",
    "RidgeModel.fit(x,y) #e.g. RigeModel.fit(x_train, y_train)\n",
    "\n",
    "#3. Step: predict \n",
    "y_hat = RidgeModel.predict(x) e.g. y_hat = RigeModel.predict(x_test)\n",
    "\n",
    "#4. Step: calculate R_squared\n",
    "R_quared = RidgeModel.score(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Grid Search\n",
    "\n",
    "is the process of finding the best hyperparameter alpha.\n",
    "\n",
    "In Python:\n",
    "\n",
    "1. Step: create a dictionary of parameter values\n",
    "\n",
    "2. Step: create a ridge regions object\n",
    "\n",
    "3. Step: create a ridge grid search object \n",
    "\n",
    "4. Step: fit the model\n",
    "\n",
    "5. Step: obtain the estimator with the best parameters\n",
    "\n",
    "6. Step: test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Step: create a dictionary of parameter values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\n",
    "\n",
    "#2. Step: create a ridge regions object\n",
    "RR=Ridge()\n",
    "\n",
    "#3. Step: create a ridge grid search object \n",
    "Grid = GridSearchCV(RR, parameters,cv=4)\n",
    "\n",
    "#4. Step: fit the model\n",
    "Grid.fit(x[['...', '...', '...', '...']], y)\n",
    "# e.g. Grid.fit(x[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y)`\n",
    "\n",
    "#5. Step: obtain the estimator with the best parameters\n",
    "BestRR=Grid.best_estimator_\n",
    "\n",
    "#6. Step: test the model\n",
    "BestRR.score(x_test[['...', '...', '...', '...']], y_test)\n",
    "# e.g. BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "a method to categorize some unknown items into a discrete set of categories or classes.\n",
    "\n",
    "Types:\n",
    "\n",
    "### 1. K-nearest Neighbors (KNN): \n",
    "\n",
    "a method for classifying cases based on their similarity to other cases.\n",
    "\n",
    "Steps:\n",
    "\n",
    "\n",
    "> \n",
    "> - Pick a value for k\n",
    ">\n",
    "> - Calculate the distance of unknown case from all cases \n",
    ">\n",
    "> - Search for the k-observations that are nearest  to the measurements of the unknown data point\n",
    ">\n",
    "> - predict the response of the unknown data point using the most popular response value from the k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Done after pre-processing steps\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=4)\n",
    "print(\"train set:\", x_train.shape, y_train.shape)\n",
    "print(\"test set:\", x_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#find the best k\n",
    "ks = 50\n",
    "mean_acc = np.zeros((ks-1))\n",
    "std_acc = np.zeros((ks-1))\n",
    "\n",
    "for n in range(1,ks):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(x_train,y_train)\n",
    "    yhat = neigh.predict(x_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test,yhat)\n",
    "    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(range(1,ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.legend(('Accuracy ', '+/- 3xstd'))\n",
    "plt.xlabel('k-values')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "k_best = mean_acc.argmax()+1\n",
    "\n",
    "print('The best accuracy was with: ', mean_acc.max(), 'with k:', k_best)\n",
    "\n",
    "#train\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors = k_best).fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "yhat = neigh.predict(x_test)\n",
    "yhat[0:5]\n",
    "\n",
    "#accuracy of the model\n",
    "print('The KNN accuracy is:', metrics.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Trees: \n",
    "\n",
    "a method that slit the training dataset into distinct nodes\n",
    "\n",
    "Steps:\n",
    "\n",
    "\n",
    "> \n",
    "> - Choose an attribuite from dataset\n",
    ">\n",
    "> - Calculate the significance of attribute in the splitting data \n",
    ">\n",
    "> - Split the data based on the value of the best attribute\n",
    ">\n",
    "> - Return to the first step\n",
    "\n",
    "Important parameters:\n",
    "\n",
    "1. Information gain (IG): is the information that can increase the level of certainty after splitting. Choose the attribute with higher IG.\n",
    "\n",
    "$$\n",
    "IG = Entropy(before split)  - Entropy(after split)\n",
    "$$\n",
    "\n",
    "Entropy = [0,1]. If Entropy = 0 => samples are completely homogeneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Done after pre-processing steps\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=4)\n",
    "print(\"train set:\", x_train.shape, y_train.shape)\n",
    "print(\"test set:\", x_test.shape, y_test.shape)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Decision Tree Accuracy\n",
    "dTree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4) #entropy is a measure of randomness or uncertainty\n",
    "\n",
    "\n",
    "#train\n",
    "dTree.fit(x_train, y_train)\n",
    "\n",
    "#predict\n",
    "predTree = dTree.predict(x_test)\n",
    "\n",
    "#accuracy of the model\n",
    "DecisionTreeAccuracy = metrics.accuracy_score(y_test,predTree)\n",
    "\n",
    "print('The Decision Tree accuracy is:', DecisionTreeAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistical Regression: \n",
    "\n",
    "a statistical method to predict the class of each customer. Logistical regression ~ linear regression but it tries to predict a categorical or discrete target field. It can be used for both binary classification and multi-class classification.\n",
    "\n",
    "when use it:\n",
    "\n",
    "- e.g. to predict the probability of a person having heart attack with a specified time period.\n",
    "- e.g. to predict the likelihood of a customer purchasing a product.\n",
    "- e.g. to predict the likelihood of a homeower defaulting on a mortgage.\n",
    "\n",
    "Steps:\n",
    "> \n",
    "> - Initialize the parameters randomly\n",
    ">\n",
    "> - Feed the cost function with training dataset, and calculate the error\n",
    ">\n",
    "> - Calculate a gradient of cost function\n",
    ">\n",
    "> - Update weights with new values\n",
    ">\n",
    "> - Go to step 2 untill cost is small enough\n",
    ">\n",
    "> - Predict the value\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Done after pre-processing steps\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=4)\n",
    "print(\"train set:\", x_train.shape, y_train.shape)\n",
    "print(\"test set:\", x_test.shape, y_test.shape)\n",
    "\n",
    "#train\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(x_train,y_train)\n",
    "\n",
    "#predict\n",
    "LR_predit = LR.predict(x_test)\n",
    "LR_prob = LR.predict_proba(x_test)\n",
    "\n",
    "#accuracy of the model\n",
    "LRAccurancy = metrics.accuracy_score(y_test,LR_predit)\n",
    "print('The Logistic Regression accuracy is:', LRAccurancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Support Vector Machine(SVM): \n",
    "\n",
    "a method that classifies cases by finding a separator.\n",
    "\n",
    "when use it:\n",
    "\n",
    "- e.g. Image recognition\n",
    "- e.g. Text category assignment\n",
    "- e.g. Detecting spam\n",
    "\n",
    "Steps:\n",
    "> \n",
    "> - Mapping data to a high-dimensional feature space\n",
    ">\n",
    "> - Finding a separator\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Done after pre-processing steps\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=4)\n",
    "print(\"train set:\", x_train.shape, y_train.shape)\n",
    "print(\"test set:\", x_test.shape, y_test.shape)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "#Modeling with SVM\n",
    "clf = svm.SVC(kernel='rbf') #Kernel is the function to mapping data into a higher-dimensional space\n",
    "\n",
    "#train\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "#predict\n",
    "SVM = clf.predict(x_test)\n",
    "\n",
    "#accuracy of the model\n",
    "SVMAccuracy = metrics.accuracy_score(y_test,SVM)\n",
    "print('The SVM accuracy is:', SVMAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "3 options:\n",
    "\n",
    "1. Jaccard index \n",
    "\n",
    "2. F1-score\n",
    "\n",
    "3. Log loss (only for Logistical Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Jaccard index (function `jaccard_similarity_score( )`)\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "values = jaccard_similarity_score(y_test, y_hat)\n",
    "\n",
    "#2. F1-score (function `f1_score( )`)\n",
    "from sklearn.metrics import f1_score\n",
    "values = f1_score(y_test, y_hat)\n",
    "\n",
    "#3. Log loss (function `log_loss( )`, only for Logistical Regression)\n",
    "from sklearn.metrics import log_loss\n",
    "values = log_loss(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:\n",
    "\n",
    "#predict different methods\n",
    "Knn_predTest = neigh.predict(X_testset)\n",
    "DTree_predTest = dTree.predict(X_testset)\n",
    "SVM_predTest = clf.predict(X_testset)\n",
    "LR_predTest = LR.predict(X_testset)\n",
    "LRp_predTest = LR.predict_proba(X_testset)\n",
    "\n",
    "#f1 score\n",
    "Knn_predTest_f1score = f1_score(y_testset, Knn_predTest)\n",
    "DTree_predTest_f1score = f1_score(y_testset, DTree_predTest)\n",
    "SVM_predTest_f1score = f1_score(y_testset, SVM_predTest)\n",
    "LR_predTest_f1score = f1_score(y_testset, LR_predTest)\n",
    "\n",
    "#Jaccard\n",
    "Knn_predTest_jaccard = jaccard_similarity_score(y_testset, Knn_predTest)\n",
    "DTree_predTest_jaccard = jaccard_similarity_score(y_testset, DTree_predTest)\n",
    "SVM_predTest_jaccard = jaccard_similarity_score(y_testset, SVM_predTest)\n",
    "LR_predTest_jaccard = jaccard_similarity_score(y_testset, LR_predTest)\n",
    "\n",
    "#Log loss\n",
    "LR_predTest_LogLoss = log_loss(y_testset, LRp_predTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "is a method of grouping data based on the similarity of the customers.\n",
    "\n",
    "when use it:\n",
    "\n",
    "- Retail/Marketing: e.g. identifying building patterns of customers, recommending new books or movies to new customers\n",
    "- Banking: e.g. fraud detection in credit card use, identifying clusters of customers\n",
    "- Insurance: e.g. fraud detection in claims analysis, insurance risk of customers\n",
    "\n",
    "Types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Partitioned-based clustering\n",
    "\n",
    "- relatively efficient\n",
    "- for medium and large size databases\n",
    "- e.g. k-mean, k-median, Fuzzy c-means\n",
    "\n",
    "### k-mean clustering:\n",
    "\n",
    "- divides the data into non-overlapping subsets (clusters) without any cluster-internal structure. \n",
    "- within a cluster, data are very similar; accross different clusters, data are very different.\n",
    "- Objective is to form clusters in such a way that similar samokes go into a cluster and dissimilar samples fall into different cluster\n",
    "- need pre-specify the number of clusters(k)\n",
    "\n",
    "Steps:\n",
    ">\n",
    "> - randomly placing k centroids, one for each cluster\n",
    "> \n",
    "> - calculate the distance of each point from each centroid\n",
    ">\n",
    "> - minimize the distance of each point from the centroid of each cluster and maximize the distance from other cluster centroids.\n",
    "> \n",
    "> - compute the new centroids for each cluster. \n",
    ">\n",
    "> - repeat until there are no more changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMean\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "%matplotlib inline\n",
    "\n",
    "#1. set up a random seed\n",
    "np.random.seed(0)\n",
    "#2.generate a set of data\n",
    "X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)\n",
    "#3. set up k_mean\n",
    "k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\n",
    "#4. fit the model\n",
    "k_means.fit(X)\n",
    "#5. grab the labels for each point in the model \n",
    "k_means_labels = k_means.labels_\n",
    "#6. get the coordinates of the cluster centers\n",
    "k_means_cluster_centers = k_means.cluster_centers_\n",
    "\n",
    "#plot\n",
    "# Initialize the plot with the specified dimensions.\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Colors uses a color map, which will produce an array of colors based on.\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n",
    "# Create a plot\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# For loop that plots the data points and centroids.\n",
    "for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):\n",
    "    # Create a list of all data points, where the data poitns that are \n",
    "    # in the cluster (ex. cluster 0) are labeled as true, else they are\n",
    "    # labeled as false.\n",
    "    my_members = (k_means_labels == k)    \n",
    "    # Define the centroid, or cluster center.\n",
    "    cluster_center = k_means_cluster_centers[k]   \n",
    "    # Plots the datapoints with color col.\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')   \n",
    "    # Plots the centroids with specified color, but with a darker outline\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n",
    "    \n",
    "# Title of the plot\n",
    "ax.set_title('KMeans')\n",
    "# Remove x-axis ticks\n",
    "ax.set_xticks(())\n",
    "# Remove y-axis ticks\n",
    "ax.set_yticks(())\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hierarchical clustering \n",
    "\n",
    "- very intuitive \n",
    "- for small size databases\n",
    "- produce trees of clusters\n",
    "- e.g. agglomerative, divisive\n",
    "\n",
    "### Agglomerative algorithm\n",
    "\n",
    "- don't need to specify the number of clusters\n",
    "- the dendrogram produced is very useful in understanding the data\n",
    "- for large dataset, it can become difficult to determine the correct number of clusters by the dendrogram\n",
    "\n",
    "Steps:\n",
    ">\n",
    "> - create n clusters, one for each data point\n",
    "> \n",
    "> - compute the proximity matrix (use different distance measurements to calculate the proximity matrix, e.g. euclidean distance)\n",
    ">\n",
    "> - repeat:\n",
    ">\n",
    ">   1. merge 2 closest clusters\n",
    "> \n",
    ">   2. update the proximity matrix\n",
    ">\n",
    "> - until only a single cluster remains\n",
    "\n",
    "Different criteria to find the closest clusters and merge them:\n",
    "\n",
    "(In general, it dipends on data type, dimensionality of data and domain knowledge of dataset)\n",
    "\n",
    "1. Single-Linkage Clustering (minimum distance between clusters)\n",
    "2. Complete-Linkage Clustering (maximum distance between clusters)\n",
    "3. Average-Linkage Clustering (average distance between clusters)\n",
    "4. Centroid-Linkage Clustering (distance between cluster centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import ndimage \n",
    "from scipy.cluster import hierarchy \n",
    "from scipy.spatial import distance_matrix \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import manifold, datasets \n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.datasets.samples_generator import make_blobs \n",
    "%matplotlib inline\n",
    "\n",
    "#1. generate a set of data\n",
    "X1, y1 = make_blobs(n_samples=50, centers=[[4,4], [-2, -1], [1, 1], [10,4]], cluster_std=0.9)\n",
    "#2. set up a agglomerative clustering\n",
    "agglom = AgglomerativeClustering(n_clusters = 4, linkage = 'average')\n",
    "#3. fit the model\n",
    "agglom.fit(X1,y1)\n",
    "#4. plot\n",
    " Create a figure of size 6 inches by 4 inches.\n",
    "plt.figure(figsize=(6,4))\n",
    "# These two lines of code are used to scale the data points down,\n",
    "# Or else the data points will be scattered very far apart.\n",
    "# Create a minimum and maximum range of X1.\n",
    "x_min, x_max = np.min(X1, axis=0), np.max(X1, axis=0)\n",
    "# Get the average distance for X1.\n",
    "X1 = (X1 - x_min) / (x_max - x_min)\n",
    "# This loop displays all of the datapoints.\n",
    "for i in range(X1.shape[0]):\n",
    "    # Replace the data points with their respective cluster value \n",
    "    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)\n",
    "    plt.text(X1[i, 0], X1[i, 1], str(y1[i]),\n",
    "             color=plt.cm.nipy_spectral(agglom.labels_[i] / 10.),\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "    \n",
    "# Remove the x ticks, y ticks, x and y axis\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "#plt.axis('off')\n",
    "\n",
    "# Display the plot of the original data before clustering\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='.')\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Dendrogram Associated for the Agglomerative Hierarchical Clustering\n",
    "dist_matrix = distance_matrix(X1,X1) \n",
    "Z = hierarchy.linkage(dist_matrix, 'complete')\n",
    "dendro = hierarchy.dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Density-based clustering\n",
    "\n",
    "- produce arbitrary shaped clusters\n",
    "- good for spatial clusters or when there is noise in databases\n",
    "- e.g. DBSCAN\n",
    "\n",
    "### DBSCAN Clustering\n",
    "\n",
    "- separate regions of high density from regions of low density (works based on density objects)\n",
    "- proper for arbitrary shape clusters, without getting effected by noise \n",
    "- for examining spatial data\n",
    "\n",
    "It works based on 2 parameters:\n",
    "\n",
    "1. R (radius of neighborhood, that include enough number of points within)\n",
    "2. M (min number of neighborhors in a neighborhood to define a cluster)\n",
    "\n",
    "Type of points:\n",
    "\n",
    "- Core point (a data point is a core point if within neghborhood of the point there are at least M points)\n",
    "- Border point (a data is a border point if within neghborhood of the point there are less than M points)\n",
    "- Outlier point \n",
    "\n",
    "Steps:\n",
    "\n",
    "- Identify all points (core, border, outlier points)\n",
    "- connect all core points and put them in the same cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.cluster import DBSCAN \n",
    "from sklearn.datasets.samples_generator import make_blobs \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "#the function generate the data points\n",
    "def createDataPoints(centroidLocation, numSamples, clusterDeviation):\n",
    "    # Create random data and store in feature matrix X and response vector y.\n",
    "    X, y = make_blobs(n_samples=numSamples, centers=centroidLocation, \n",
    "                                cluster_std=clusterDeviation)\n",
    "    \n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "X, y = createDataPoints([[4,3], [2,-1], [-1,4]] , 1500, 0.5)\n",
    "\n",
    "#Model\n",
    "epsilon = 0.3 #determine a specified radius\n",
    "minimumSamples = 7\n",
    "db = DBSCAN(eps=epsilon, min_samples=minimumSamples).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "# Firts, create an array of booleans using the labels from db.\n",
    "#Replace all elements with 'True' in core_samples_mask that are in the cluster, 'False' if the points are outliers.\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# Remove repetition in labels by turning it into a set.\n",
    "unique_labels = set(labels)\n",
    "# Create colors for the clusters.\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "# Plot the points with colors\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    # Plot the datapoints that are clustered\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.5)\n",
    "\n",
    "    # Plot the outliers\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
